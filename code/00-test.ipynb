{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e8389-2128-4f77-aa63-9ef4bda60700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.utils import from_networkx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import random\n",
    "seed = 42\n",
    "torch.manual_seed(seed)       # PyTorch\n",
    "random.seed(seed)             # Python's random module\n",
    "np.random.seed(seed)  \n",
    "\n",
    "class NodeRankingGNN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim):\n",
    "        super(NodeRankingGNN, self).__init__()\n",
    "        self.conv1 = pyg_nn.GCNConv(in_features, hidden_dim)\n",
    "        self.conv2 = pyg_nn.GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = pyg_nn.GCNConv(hidden_dim, 1)  # Output ranking score\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x  # No sigmoid to keep ranking raw\n",
    "\n",
    "class CentralityMetric:\n",
    "    def __init__(self, g, layer1, layer2, new):\n",
    "        self.new = new\n",
    "        self.G = g  # Replace with your actual graph\n",
    "        self.g1 = layer1\n",
    "        self.g2 = layer2\n",
    "        self.nodes = list(self.G.nodes())\n",
    "        self.nodes.sort()\n",
    "    \n",
    "        # Ensure the graph is directed\n",
    "        if not self.G.is_directed():\n",
    "            print(\"Converting to a directed graph.\")\n",
    "            self.G = nx.DiGraph(self.G)\n",
    "        \n",
    "        # Use the largest weakly connected component\n",
    "        if not nx.is_weakly_connected(self.G):\n",
    "            print(\"Graph is not weakly connected. Using the largest weakly connected component.\")\n",
    "            self.largest_component = max(nx.weakly_connected_components(self.G), key=len)\n",
    "            self.G = self.G.subgraph(self.largest_component).copy()\n",
    "        \n",
    "        # Choose a source node with the highest out-degree\n",
    "        self.source = max(self.nodes, key=lambda n: self.G.out_degree(n))\n",
    "        print(f\"Source node: {self.source} (Out-degree: {self.G.out_degree(self.source)})\")\n",
    "\n",
    "    def compMLModularity(self, g, layer1, layer2, gamma=1, omega=1):\n",
    "        layer1 = [x for x in layer1.nodes()]\n",
    "        layer1.sort()\n",
    "        layer2 = [x for x in layer2.nodes()]\n",
    "        layer2.sort()\n",
    "        # a = self.a+self.c\n",
    "        a = nx.adjacency_matrix(g).todense()\n",
    "        m = a.sum() / 2  \n",
    "        commu = list(greedy_modularity_communities(g))\n",
    "        commuMap = {node: i for i,com in enumerate(commu) for node in com}\n",
    "        mod = {i: 0 for i in range(len(a))}  \n",
    "    \n",
    "        for i in range(len(a)):\n",
    "            for j in range(len(a)):\n",
    "                same = commuMap[i] == commuMap[j]\n",
    "                intra = (i in layer1 and j in layer1) or (i in layer2 and j in layer2)\n",
    "                inter = (i in layer1 and j in layer2) or (i in layer2 and j in layer1)\n",
    "                ki = a[i].sum()\n",
    "                kj = a[j].sum()\n",
    "        \n",
    "                if intra:\n",
    "                    contrib = (a[i, j] - gamma * (ki * kj / (2 * m))) * same\n",
    "                    mod[i] += contrib  \n",
    "                    mod[j] += contrib  \n",
    "        \n",
    "                elif inter:\n",
    "                    contrib = omega * a[i, j] * same\n",
    "                    mod[i] += contrib  \n",
    "                    mod[j] += contrib  \n",
    "        \n",
    "        return(mod)\n",
    "\n",
    "    def compute(self):\n",
    "        k_core = nx.core_number(self.G)\n",
    "        core_values = list(k_core.values())\n",
    "        min_core = min(core_values)\n",
    "        max_core = max(core_values)\n",
    "        normalized_k_core = {node: (core - min_core) / (max_core - min_core) for node, core in k_core.items()}\n",
    "        # normalized_k_core = [normalized_k_core[x] for x in self.nodes]\n",
    "        \n",
    "        degree_centrality = nx.degree_centrality(self.G)\n",
    "        values = list(degree_centrality.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        degree_centrality = {key: (value - min_val) / (max_val - min_val) for key, value in degree_centrality.items()}\n",
    "        \n",
    "        closeness_centrality = nx.closeness_centrality(self.G)\n",
    "        values = list(closeness_centrality.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        closeness_centrality = {key: (value - min_val) / (max_val - min_val) for key, value in closeness_centrality.items()}\n",
    "        \n",
    "        eigenvector_centrality = nx.eigenvector_centrality_numpy(self.G)\n",
    "        values = list(eigenvector_centrality.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        eigenvector_centrality = {key: (value - min_val) / (max_val - min_val) for key, value in eigenvector_centrality.items()}\n",
    "        \n",
    "        betweenness_centrality = nx.betweenness_centrality(self.G)\n",
    "        values = list(betweenness_centrality.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        betweenness_centrality = {key: (value - min_val) / (max_val - min_val) for key, value in betweenness_centrality.items()}\n",
    "        \n",
    "        # pagerank = nx.pagerank(G)\n",
    "        # values = list(pagerank.values())\n",
    "        # min_val = min(values)\n",
    "        # max_val = max(values)\n",
    "        # pagerank = {key: (value - min_val) / (max_val - min_val) for key, value in pagerank.items()}\n",
    "        \n",
    "        clustering_coeff = nx.clustering(G.to_undirected())\n",
    "        values = list(clustering_coeff.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        clustering_coeff = {key: (value - min_val) / (max_val - min_val) for key, value in clustering_coeff.items()}\n",
    "        \n",
    "        l3_metric = self.compute_l3_triangle_metric(self.G)\n",
    "        values = list(l3_metric.values())\n",
    "        min_val = min(values)\n",
    "        max_val = max(values)\n",
    "        l3_metric = {key: (value - min_val) / (max_val - min_val) for key, value in l3_metric.items()}\n",
    "\n",
    "        mc = MultiCens(g, layers, .9, 1000)\n",
    "        m = mc.getMultiCens()\n",
    "        \n",
    "        # mod_score = self.compMLModularity(self.G, self.g1, self.g2)\n",
    "        # values = list(mod_score.values())\n",
    "        # min_val = min(values)\n",
    "        # max_val = max(values)\n",
    "        # mod_score = {key: (value - min_val) / (max_val - min_val) for key, value in mod_score.items()}\n",
    "        \n",
    "        \n",
    "        immediate_dominators = nx.immediate_dominators(self.G, self.source)\n",
    "        # Compute transitive dominator influence\n",
    "        dominator_count = {node: 0 for node in self.nodes}\n",
    "        for node in self.nodes:\n",
    "            if node in immediate_dominators:\n",
    "                dom = immediate_dominators[node]\n",
    "                if dom in dominator_count:\n",
    "                    dominator_count[dom] += 1\n",
    "        \n",
    "        # Fix: Compute transitive closure of domination\n",
    "        for node in self.nodes:\n",
    "            dominated_set = set()\n",
    "            stack = [node]\n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if current in immediate_dominators and current != self.source:\n",
    "                    parent = immediate_dominators[current]\n",
    "                    if parent not in dominated_set:\n",
    "                        dominated_set.add(parent)\n",
    "                        stack.append(parent)\n",
    "            \n",
    "            # The number of nodes dominated (excluding itself)\n",
    "            dominator_count[node] = len(dominated_set)\n",
    "        dominator_weight = {node: dominator_count[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        dominator_centrality = nx.pagerank(self.G, personalization=dominator_weight)\n",
    "        kcore_weight = {node: normalized_k_core[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        kcore = nx.pagerank(self.G, personalization=kcore_weight)\n",
    "        btw_weight = {node: betweenness_centrality[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        btw = nx.pagerank(self.G, personalization=btw_weight)\n",
    "        l3_weight = {node: l3_metric[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        l3 = nx.pagerank(self.G, personalization=l3_weight)\n",
    "        deg_weight = {node: degree_centrality[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        deg = nx.pagerank(self.G, personalization=deg_weight)\n",
    "        mc_weight = {node: m[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        mc = nx.pagerank(self.G, personalization=mc_weight)\n",
    "        # mod_weight = {node: mod_score[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        # mod = nx.pagerank(self.G, personalization=mod_weight)\n",
    "\n",
    "        new_weight = {node: self.new[node] for node in self.nodes}  # Bias higher for dominators\n",
    "        neww = nx.pagerank(self.G, personalization=new_weight)\n",
    "        \n",
    "        # print(dominator_centrality)\n",
    "        \n",
    "        \n",
    "        # --- Step 3: Convert Features to PyTorch Tensor ---\n",
    "        # nodes = list(G.nodes())  # Maintain order\n",
    "        \n",
    "        node_features = []\n",
    "        for node in self.nodes:\n",
    "            node_features.append([\n",
    "                # degree_centrality[node],\n",
    "                # closeness_centrality[node],\n",
    "                # betweenness_centrality[node],\n",
    "                # eigenvector_centrality[node],\n",
    "                # pagerank[node],\n",
    "                # clustering_coeff[node],\n",
    "                # mod[node],\n",
    "                # m[node],\n",
    "                # deg[node],\n",
    "                # self.new[node],\n",
    "                # neww[node],\n",
    "                # l3_metric[node],\n",
    "                # l3[node],\n",
    "                # dominator_count[node],\n",
    "                # dominator_centrality[node],  # Add dominator centrality as a feature\n",
    "                # normalized_k_core[node],\n",
    "                # kcore[node]\n",
    "                # btw[node],\n",
    "                # mc[node]\n",
    "            ])\n",
    "        \n",
    "        node_features = torch.tensor(node_features, dtype=torch.float32)\n",
    "        \n",
    "        # Convert to PyG format\n",
    "        data = from_networkx(self.G)\n",
    "        data.x = node_features  # Assign node features\n",
    "        \n",
    "        # Create a node-to-index mapping\n",
    "        node_to_index = {node: i for i, node in enumerate(self.nodes)}\n",
    "\n",
    "        # wt = []\n",
    "        # for node in sorted(node_to_index.keys()):\n",
    "        #     wt.append(dominator_weight[node])\n",
    "        # wt = torch.tensor(wt)\n",
    "        # Initialize Model\n",
    "        model = NodeRankingGNN(in_features=node_features.shape[1], hidden_dim=16)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "        for epoch in range(100):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            rankings = model(data.x, data.edge_index).squeeze()\n",
    "            transmission = self.refined_transmission(self.G, rankings, dominator_centrality)\n",
    "            loss = -torch.sum(transmission) + self.ranking_loss(rankings, data.edge_index)\n",
    "            # loss = -torch.sum(transmission) + self.ranking_loss(rankings, data.edge_index, wt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print(f\"Epoch {epoch+1}: Loss = {loss.item()}\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            rankings = model(data.x, data.edge_index).squeeze()\n",
    "        rankings = (rankings - rankings.mean()) / rankings.std()\n",
    "        rankings_np = rankings.cpu().numpy()\n",
    "        \n",
    "        metric = []\n",
    "        d, c, e, b = [], [], [], []\n",
    "        # print(\"Node Rankings:\")\n",
    "        # for node in sorted(node_to_index.keys()):\n",
    "        # for node in self.nodes:\n",
    "            # print(f\"Node {node}: Score = {rankings_np[node_to_index[node]]:.4f}\")\n",
    "            # metric.append(rankings_np[node_to_index[node]])\n",
    "            # d.append(degree_centrality[node_to_index[node]])\n",
    "            # c.append(closeness_centrality[node_to_index[node]])\n",
    "            # e.append(eigenvector_centrality[node_to_index[node]])\n",
    "            # b.append(betweenness_centrality[node_to_index[node]])\n",
    "            \n",
    "        \n",
    "        # metric = np.array(metric)\n",
    "        metric = rankings_np\n",
    "        metric = (metric - metric.min()) / (metric.max() - metric.min())\n",
    "        # print(metric)\n",
    "        metric = {x:metric[x] for x in self.nodes}\n",
    "        return(metric)\n",
    "    \n",
    "    # --- Step 5: Refined Transmission Model ---\n",
    "    def refined_transmission(self, G, rankings, dom_centrality, alpha=0.9, beta=0.1, num_steps=30):\n",
    "        num_nodes = rankings.shape[0]\n",
    "        transmission = torch.zeros_like(rankings)\n",
    "        edge_index = torch.tensor(list(G.edges()), dtype=torch.long, device=rankings.device)\n",
    "        weights = torch.tensor([G[u][v].get('weight', 1.0) for u, v in G.edges()],\n",
    "                               dtype=torch.float32, device=rankings.device)\n",
    "        dom_centrality_tensor = torch.tensor([dom_centrality[node] for node in self.nodes], \n",
    "                                             dtype=torch.float32, device=rankings.device)\n",
    "        for _ in range(num_steps):\n",
    "            temp_transmission = torch.zeros_like(rankings)\n",
    "            influence = weights * F.sigmoid(rankings[edge_index[:, 0]]) * dom_centrality_tensor[edge_index[:, 0]]\n",
    "    \n",
    "            temp_transmission.index_add_(0, edge_index[:, 1], influence)\n",
    "    \n",
    "            # Apply decay and update transmission\n",
    "            transmission = beta * transmission + alpha * temp_transmission\n",
    "    \n",
    "        return transmission\n",
    "    \n",
    "    def ranking_loss(self, scores, edge_index):\n",
    "        src, dst = edge_index\n",
    "        diff = scores[src] - scores[dst]  # Encourage src > dst\n",
    "        return torch.mean(torch.relu(1 - diff))  # Hinge loss\n",
    "\n",
    "    # def ranking_loss(self, scores, edge_index, weights, margin=1.0, lambda_contrast=0.1, lambda_smooth=0.05):\n",
    "    #     src, dst = edge_index\n",
    "    \n",
    "    #     # Adaptive margin based on node centralities (degree-aware margin example)\n",
    "    #     margin_uv = torch.log1p(weights[src]) / torch.log1p(weights[dst])\n",
    "        \n",
    "    #     # Hinge loss with adaptive margin\n",
    "    #     loss_rank = torch.mean(torch.relu(margin_uv - (scores[src] - scores[dst])))\n",
    "    \n",
    "    #     # Contrastive separation loss\n",
    "    #     loss_contrast = torch.mean(torch.relu(margin - torch.abs(scores[src] - scores[dst])))\n",
    "    \n",
    "    #     # Graph-aware smoothness loss\n",
    "    #     loss_smooth = torch.mean(weights * (scores[src] - scores[dst]) ** 2)\n",
    "    \n",
    "    #     return loss_rank + lambda_contrast * loss_contrast + lambda_smooth * loss_smooth\n",
    "\n",
    "    \n",
    "    def compute_l3_triangle_metric(self, G):\n",
    "        l3_triangle_metric = {}\n",
    "    \n",
    "        for node in self.nodes:\n",
    "            # Find all 2-hop neighbors\n",
    "            neighbors = set(G.neighbors(node))\n",
    "            # Get all 3-hop neighbors\n",
    "            three_hop_neighbors = set()\n",
    "    \n",
    "            for neighbor in neighbors:\n",
    "                three_hop_neighbors.update(G.neighbors(neighbor))\n",
    "    \n",
    "            # Count triangles involving the node and its 2-hop and 3-hop neighbors\n",
    "            triangle_count = 0\n",
    "            for three_hop_neighbor in three_hop_neighbors:\n",
    "                if node in G.neighbors(three_hop_neighbor):\n",
    "                    triangle_count += 1\n",
    "    \n",
    "            l3_triangle_metric[node] = triangle_count\n",
    "    \n",
    "        return l3_triangle_metric\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a3ca1-5c3b-49ef-bfb9-a31c1bd3ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from torch_geometric.utils import from_networkx\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "class NodeEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_nodes, feature_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = torch.nn.Parameter(torch.randn(num_nodes, feature_dim) * 0.1)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.embeddings\n",
    "\n",
    "# Prepare graph data\n",
    "def prepare_graph_data(G1, G2, G):\n",
    "    data_G1 = from_networkx(G1)\n",
    "    data_G2 = from_networkx(G2)\n",
    "    data_G = from_networkx(G)  \n",
    "    num_nodes_G1 = data_G1.num_nodes if data_G1.num_nodes is not None else data_G1.x.shape[0]\n",
    "    num_nodes_G2 = data_G2.num_nodes if data_G2.num_nodes is not None else data_G2.x.shape[0]\n",
    "    num_nodes_G = data_G.num_nodes if data_G.num_nodes is not None else data_G.x.shape[0]\n",
    "    feature_dim = max(data_G1.x.shape[1] if data_G1.x is not None else 0,\n",
    "                  data_G2.x.shape[1] if data_G2.x is not None else 0,\n",
    "                  data_G.x.shape[1] if data_G.x is not None else 0)\n",
    "\n",
    "    if data_G1.x is None:\n",
    "        # data_G1.x = torch.eye(num_nodes_G1, feature_dim)\n",
    "        node_emb1 = NodeEmbedding(num_nodes_G1, feature_dim)\n",
    "        data_G1.x = node_emb1()\n",
    "    if data_G2.x is None:\n",
    "        # data_G2.x = torch.eye(num_nodes_G2, feature_dim)\n",
    "        node_emb2 = NodeEmbedding(num_nodes_G2, feature_dim)\n",
    "        data_G2.x = node_emb2()\n",
    "    if data_G.x is None:\n",
    "        # data_G.x = torch.eye(num_nodes_G, feature_dim)\n",
    "        node_emb = NodeEmbedding(num_nodes_G, feature_dim)\n",
    "        data_G.x = node_emb()\n",
    "    # if data_G1.x is None:\n",
    "    #     data_G1.x = torch.randn(num_nodes_G1, feature_dim)  # Random features\n",
    "    # if data_G2.x is None:\n",
    "    #     data_G2.x = torch.randn(num_nodes_G2, feature_dim)  # Random features\n",
    "    # if data_G.x is None:\n",
    "    #     data_G.x = torch.randn(num_nodes_G, feature_dim) \n",
    "    assert data_G1.num_features == data_G2.num_features == data_G.num_features\n",
    "\n",
    "    return data_G1, data_G2, data_G\n",
    "\n",
    "G1 = g1\n",
    "G2 = g2\n",
    "G = g\n",
    "data_G1, data_G2, data_G = prepare_graph_data(G1, G2, G)\n",
    "for data in [data_G1, data_G2, data_G]:\n",
    "    data.edge_index, _ = remove_self_loops(data.edge_index)\n",
    "    data.edge_index, _ = add_self_loops(data.edge_index)\n",
    "\n",
    "print(\"G1 edges:\", G1.number_of_edges())\n",
    "print(\"G2 edges:\", G2.number_of_edges())\n",
    "print(\"G edges:\", G.number_of_edges())\n",
    "\n",
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        self.conv3 = GCNConv(64, 64)\n",
    "        self.conv4 = GCNConv(64, out_channels)  # Added one more layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x1 = self.conv1(x, edge_index).relu()\n",
    "        x2 = x1 + self.conv2(x1, edge_index).relu()  \n",
    "        x3 = x2 + self.conv3(x2, edge_index).relu()  \n",
    "        x4 = x3 + self.conv4(x3, edge_index)  \n",
    "        return x4\n",
    "        \n",
    "# Define the model\n",
    "out_channels = 64  # Embedding dimension\n",
    "encoder_g1 = GNNEncoder(data_G1.num_features, out_channels)\n",
    "encoder_g2 = GNNEncoder(data_G2.num_features, out_channels)\n",
    "\n",
    "class GAEModel(torch.nn.Module):\n",
    "    def __init__(self, encoder_g1, encoder_g2):\n",
    "        super().__init__()\n",
    "        self.encoder_g1 = encoder_g1\n",
    "        self.encoder_g2 = encoder_g2\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * out_channels, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_G1, data_G2):\n",
    "        z1 = self.encoder_g1(data_G1.x, data_G1.edge_index)\n",
    "        z2 = self.encoder_g2(data_G2.x, data_G2.edge_index)\n",
    "        z = torch.cat([z1, z2], dim=0)\n",
    "        return z\n",
    "    \n",
    "    def recon_loss(self, z, edge_index):\n",
    "        # Positive edges\n",
    "        pos_edge_predictions = self.decoder(torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=1))\n",
    "        pos_edge_predictions = torch.sigmoid(pos_edge_predictions)\n",
    "    \n",
    "        # Negative edges (increase the number of negative samples)\n",
    "        neg_edge_index = negative_sampling(edge_index, num_neg_samples=edge_index.size(1) * 2, num_nodes=z.size(0))\n",
    "        neg_edge_predictions = self.decoder(torch.cat([z[neg_edge_index[0]], z[neg_edge_index[1]]], dim=1))\n",
    "        neg_edge_predictions = torch.sigmoid(neg_edge_predictions)\n",
    "    \n",
    "        # Loss\n",
    "        pos_loss = F.binary_cross_entropy(pos_edge_predictions, torch.ones_like(pos_edge_predictions))\n",
    "        neg_loss = F.binary_cross_entropy(neg_edge_predictions, torch.zeros_like(neg_edge_predictions))\n",
    "        print(\"Pos Loss:\", pos_loss.item())\n",
    "        print(\"Neg Loss:\", neg_loss.item())\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "\n",
    "model = GAEModel(encoder_g1, encoder_g2)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(data_G1, data_G2)\n",
    "    loss = model.recon_loss(z, data_G.edge_index)\n",
    "    # loss += 0.01 * torch.sum(z * torch.log(z + 1e-10)) \n",
    "    # loss += 0.001 * torch.sum(z * torch.log(z + 1e-10))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    loss = train()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Evaluate node importance\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    z = model(data_G1, data_G2)\n",
    "\n",
    "\n",
    "    # # Node importance: Norm of embeddings\n",
    "    # node_scores = torch.norm(z, dim=1)\n",
    "    # print(\"Embedding Variance:\", torch.var(z, dim=0).mean().item())\n",
    "    # z = (z - z.mean(dim=0)) / (z.std(dim=0) + 1e-10)\n",
    "    # print(\"Embedding Variance:\", torch.var(z, dim=0).mean().item())\n",
    "    # print(\"Embedding Mean:\", torch.mean(z, dim=0))\n",
    "    # print(\"Embedding STD:\", torch.std(z, dim=0))\n",
    "    # node_probs = F.softmax(z, dim=1)  # Convert to probability-like distribution\n",
    "    # print(\"Min node prob:\", node_probs.min().item())\n",
    "    # print(\"Max node prob:\", node_probs.max().item())\n",
    "    # print(\"Mean node prob:\", node_probs.mean().item())\n",
    "    # print(\"Variance of node probs:\", torch.var(node_probs, dim=0).mean().item())\n",
    "    # node_scores = -torch.sum(node_probs * torch.log(node_probs + 1e-10), dim=1)  # Shannon entropy\n",
    "    z = (z - z.mean(dim=0)) / (z.std(dim=0) + 1e-10)  # Normalize embeddings\n",
    "    print(\"Embeddings:\", z)\n",
    "    print(\"Embedding variance:\", torch.var(z, dim=0).mean().item())\n",
    "    print(\"Embedding mean:\", torch.mean(z, dim=0))\n",
    "    node_scores = -torch.sum(F.softmax(z, dim=1) * torch.log(F.softmax(z, dim=1) + 1e-10), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "node_labels_G1 = sorted(G1.nodes())\n",
    "node_labels_G2 = sorted(G2.nodes())\n",
    "\n",
    "# Combine node labels\n",
    "node_labels = node_labels_G1 + node_labels_G2\n",
    "\n",
    "# Combine node labels and scores\n",
    "node_importance = list(zip(node_labels, node_scores.tolist()))\n",
    "\n",
    "# Sort by score (descending) to get the most important nodes\n",
    "node_importance_sorted = sorted(node_importance, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print top 10 important nodes\n",
    "print(\"Top 10 important nodes:\")\n",
    "for node, score in node_importance_sorted[:10]:\n",
    "    print(f\"Node: {node}, Score: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
