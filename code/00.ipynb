{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f024eb40-acb7-49a3-aadf-a3b31debf255",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Cross-Attention Module\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCrossAttention\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Cross-Attention Module\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.query_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # Compute attention weights\n",
    "        query = self.query_layer(query)\n",
    "        key = self.key_layer(key)\n",
    "        value = self.value_layer(value)\n",
    "        attention_weights = self.softmax(torch.matmul(query, key.transpose(-2, -1)))\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_features = torch.matmul(attention_weights, value)\n",
    "        return attended_features\n",
    "\n",
    "# Image Encoder\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        self.cnn.fc = nn.Identity()\n",
    "        self.fc = nn.Linear(512, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Gene Encoder\n",
    "class GeneEncoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GeneEncoder, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseMultiModalModel(nn.Module):\n",
    "    def __init__(self, gene_input_dim):\n",
    "        super(SiameseMultiModalModel, self).__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.gene_encoder = GeneEncoder(gene_input_dim)\n",
    "        self.cross_attention = CrossAttention(embed_dim=256)\n",
    "        self.bilinear_pooling = nn.Bilinear(256, 256, 128)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "\n",
    "    def forward(self, image1, gene1, image2, gene2):\n",
    "        # Encode first pair\n",
    "        img_features1 = self.image_encoder(image1)\n",
    "        gene_features1 = self.gene_encoder(gene1)\n",
    "\n",
    "        # Encode second pair\n",
    "        img_features2 = self.image_encoder(image2)\n",
    "        gene_features2 = self.gene_encoder(gene2)\n",
    "\n",
    "        # Cross-attention for first pair\n",
    "        attended_img1 = self.cross_attention(gene_features1.unsqueeze(1), \n",
    "                                             img_features1.unsqueeze(1), \n",
    "                                             img_features1.unsqueeze(1)).squeeze(1)\n",
    "        attended_gene1 = self.cross_attention(img_features1.unsqueeze(1), \n",
    "                                              gene_features1.unsqueeze(1), \n",
    "                                              gene_features1.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Cross-attention for second pair\n",
    "        attended_img2 = self.cross_attention(gene_features2.unsqueeze(1), \n",
    "                                             img_features2.unsqueeze(1), \n",
    "                                             img_features2.unsqueeze(1)).squeeze(1)\n",
    "        attended_gene2 = self.cross_attention(img_features2.unsqueeze(1), \n",
    "                                              gene_features2.unsqueeze(1), \n",
    "                                              gene_features2.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Bilinear pooling for both pairs\n",
    "        fused_features1 = self.bilinear_pooling(attended_img1, attended_gene1)\n",
    "        fused_features2 = self.bilinear_pooling(attended_img2, attended_gene2)\n",
    "\n",
    "        # Project to shared embedding space\n",
    "        embed1 = self.projection_head(fused_features1)\n",
    "        embed2 = self.projection_head(fused_features2)\n",
    "\n",
    "        return embed1, embed2\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embed1, embed2, label):\n",
    "        # Compute pairwise distance\n",
    "        distance = nn.functional.pairwise_distance(embed1, embed2)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        loss = torch.mean((1 - label) * torch.pow(distance, 2) + \n",
    "                          label * torch.pow(torch.clamp(self.margin - distance, min=0.0), 2))\n",
    "        return loss\n",
    "\n",
    "import random\n",
    "import torch.utils.data as data\n",
    "\n",
    "class PatchDataset(data.Dataset):\n",
    "    def __init__(self, patches, gene_data, labels, max_patches_per_patient=10):\n",
    "        self.patches = patches  # Dict with patient_id -> list of image patches\n",
    "        self.gene_data = gene_data  # Dict with patient_id -> gene expression\n",
    "        self.labels = labels  # Dict with patient_id -> label\n",
    "        self.max_patches_per_patient = max_patches_per_patient\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patient_id = list(self.patches.keys())[idx]\n",
    "        all_patches = self.patches[patient_id]\n",
    "        \n",
    "        # Randomly sample patches\n",
    "        if len(all_patches) > self.max_patches_per_patient:\n",
    "            selected_patches = random.sample(all_patches, self.max_patches_per_patient)\n",
    "        else:\n",
    "            selected_patches = all_patches\n",
    "        \n",
    "        patches_tensor = torch.stack(selected_patches)  # Stack patches\n",
    "        gene_tensor = self.gene_data[patient_id]\n",
    "        label = self.labels[patient_id]\n",
    "        \n",
    "        return patches_tensor, gene_tensor, label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c72c9-0b3f-4abc-8b63-6120b551c1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class HistopathologyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "dataset = HistopathologyDataset(image_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "metadata = pd.read_csv(\"metadata.csv\")\n",
    "metadata = pd.get_dummies(metadata, columns=[\"Subtype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7bbfb7-c87d-48ed-896d-58979c25f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Initialize model, optimizer, and scheduler\n",
    "model = MultiModalWithMetadata(gene_input_dim=10000, metadata_dim=10)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)  # Reduce LR by 0.5 every 5 epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20):  # Adjust number of epochs\n",
    "    for images, genes, metadata, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, genes, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()  # Update learning rate\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bb2eee-90d7-49f0-9aee-e54cbc3a3f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "def generate_gradcam(image, model, target_layer):\n",
    "    gradcam = GradCAM(model=model, target_layer=target_layer, use_cuda=True)\n",
    "    heatmap = gradcam(image)\n",
    "    return heatmap\n",
    "\n",
    "import shap\n",
    "\n",
    "# SHAP values for gene encoder\n",
    "explainer = shap.DeepExplainer(model.gene_encoder, gene_data_sample)\n",
    "shap_values = explainer.shap_values(gene_test_data)\n",
    "shap.summary_plot(shap_values, gene_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c984f2-6aa5-44f5-ad61-4c1a33faa56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5fed2b-6ce4-4853-bc15-6d589074144c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591b284-6baf-43bf-a714-f69be20c28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb2105-060c-4791-97fd-aa70324b86f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a30e2a-99a9-456b-94eb-9802cff2a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135f611-b436-4687-bb94-35cdd18dda51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cff3c853-f257-45eb-a916-5ece474afe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Patients: 100%|███████████████████| 100/100 [00:00<00:00, 103.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patients: 100\n",
      "patient: TCGA-A2-A0ER-01\n",
      "image features: torch.Size([1, 3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process images\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_path = \"/home/ramanuja-simha/Downloads/28050083/TCGA-BRCA-A2-DEEPMED-TILES/BLOCKS_NORM_MACENKO/\"  \n",
    "image_size = (256, 256)  \n",
    "max_patches_per_patient = 1 \n",
    "mean = [0.485, 0.456, 0.406]  \n",
    "std = [0.229, 0.224, 0.225]   \n",
    "all_patient_data_image = {}\n",
    "patient_folders = os.listdir(base_path)\n",
    "\n",
    "for patient in tqdm(patient_folders, desc=\"Processing Patients\"):\n",
    "    patient_path = os.path.join(base_path, patient)\n",
    "    patient = patient.split(\"-\")[:3]\n",
    "    patient = \"-\".join(patient)\n",
    "    patient += \"-01\"\n",
    "    if not os.path.isdir(patient_path):\n",
    "        continue\n",
    "    \n",
    "    processed_slices = []\n",
    "    slice_files = os.listdir(patient_path)\n",
    "    for slice_file in slice_files[:max_patches_per_patient]:\n",
    "        slice_path = os.path.join(patient_path, slice_file)\n",
    "        try:\n",
    "            img = cv2.imread(slice_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)                  \n",
    "            img = cv2.resize(img, image_size)\n",
    "            img = img / 255.0\n",
    "            img = (img - mean) / std\n",
    "            img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
    "            processed_slices.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {slice_path}: {e}\")\n",
    "            continue\n",
    "    if processed_slices:\n",
    "        all_patient_data_image[patient] = torch.stack(processed_slices)  \n",
    "print(\"patients:\",len(all_patient_data_image))\n",
    "print(\"patient:\",next(iter(all_patient_data_image.keys())))\n",
    "print(\"image features:\",all_patient_data_image[next(iter(all_patient_data_image.keys()))].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aecaac6-be0f-4f56-bb11-0cf7f79eb70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene expr: (20530, 1218)\n",
      "patient ids: 100\n",
      "filter: (100, 20530)\n",
      "imputer: (100, 20530)\n",
      "scaler: (100, 20530)\n",
      "selector: (100, 19997)\n",
      "pca: (100, 100)\n",
      "patients: 100\n",
      "patient: TCGA-A2-A0ER-01\n",
      "gene expr features: 100\n"
     ]
    }
   ],
   "source": [
    "# process gene expr profiling\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "file_path = \"/home/ramanuja-simha/Downloads/28050083/TCGA-BRCA-RNA-Seq.csv\"\n",
    "gene_expression_df = pd.read_csv(file_path, sep=\"\\t\", header=0, index_col=0)  \n",
    "print(\"gene expr:\",gene_expression_df.shape)\n",
    "\n",
    "patient_ids = list(all_patient_data_image.keys())\n",
    "print(\"patient ids:\",len(patient_ids))\n",
    "\n",
    "filtered_gene_expression_df = gene_expression_df[patient_ids]\n",
    "transposed_df = filtered_gene_expression_df.T\n",
    "print(\"filter:\",transposed_df.shape)\n",
    "\n",
    "row_ids = transposed_df.index.tolist()\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "gene_data_imputed = imputer.fit_transform(transposed_df)\n",
    "print(\"imputer:\",gene_data_imputed.shape)\n",
    "scaler = StandardScaler()\n",
    "gene_data_normalized = scaler.fit_transform(gene_data_imputed)\n",
    "print(\"scaler:\",gene_data_normalized.shape)\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "gene_data_selected = selector.fit_transform(gene_data_normalized)\n",
    "print(\"selector:\",gene_data_selected.shape)\n",
    "pca = PCA(n_components=100)\n",
    "gene_data_reduced = pca.fit_transform(gene_data_selected)\n",
    "print(\"pca:\",gene_data_reduced.shape)\n",
    "gene_data_reduced = pd.DataFrame(gene_data_reduced, index=row_ids)\n",
    "\n",
    "all_patient_data_geneexpr = gene_data_reduced.to_dict(orient='index')\n",
    "print(\"patients:\",len(all_patient_data_geneexpr))\n",
    "print(\"patient:\",next(iter(all_patient_data_geneexpr.keys())))\n",
    "print(\"gene expr features:\",len(all_patient_data_geneexpr[next(iter(all_patient_data_geneexpr.keys()))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4dd47a0-5ee7-4d1d-8913-d7635928d278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "{0, 1}\n",
      "Remaining patient IDs in all_patient_data: 93\n",
      "Remaining patient IDs in all_patient_data_image: 93\n",
      "Remaining patient IDs in all_patient_data_geneexpr: 93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata_path = \"/home/ramanuja-simha/Downloads/28050083/TCGA-BRCA-A2-target_variable.xlsx\"\n",
    "metadata_df = pd.read_excel(metadata_path, engine='openpyxl')\n",
    "col2_col4_dict = dict(zip(metadata_df.iloc[:, 1], metadata_df.iloc[:, 3]))\n",
    "all_patient_data = {key: col2_col4_dict[key] for key in col2_col4_dict if key in all_patient_data_image}\n",
    "print(len(all_patient_data))\n",
    "print(set(all_patient_data.values()))\n",
    "# Identify the patient IDs with class \"x\" in all_patient_data\n",
    "patients_to_remove = [patient_id for patient_id, class_label in all_patient_data.items() if class_label == \"x\"]\n",
    "\n",
    "# Remove these patient IDs from all three dictionaries\n",
    "for patient_id in patients_to_remove:\n",
    "    # Remove from all_patient_data\n",
    "    if patient_id in all_patient_data:\n",
    "        del all_patient_data[patient_id]\n",
    "    \n",
    "    # Remove from all_patient_data_image\n",
    "    if patient_id in all_patient_data_image:\n",
    "        del all_patient_data_image[patient_id]\n",
    "    \n",
    "    # Remove from all_patient_data_geneexpr\n",
    "    if patient_id in all_patient_data_geneexpr:\n",
    "        del all_patient_data_geneexpr[patient_id]\n",
    "\n",
    "# Verify the changes\n",
    "print(f\"Remaining patient IDs in all_patient_data: {len(all_patient_data.keys())}\")\n",
    "print(f\"Remaining patient IDs in all_patient_data_image: {len(all_patient_data_image.keys())}\")\n",
    "print(f\"Remaining patient IDs in all_patient_data_geneexpr: {len(all_patient_data_geneexpr.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3e56f55-6b16-49bb-9512-d60d1c43b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch import nn, optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageModel, self).__init__()\n",
    "        # Assuming input image size is 256x256 with 3 channels\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # Convolution layer for feature extraction\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Additional convolution\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 512)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Output layer for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape is (batch_size, num_patches, 3, 256, 256)\n",
    "        batch_size, num_patches, c, h, w = x.shape\n",
    "        \n",
    "        # Process each patch separately through convolution layers\n",
    "        patch_features = []\n",
    "        for i in range(num_patches):\n",
    "            patch = x[:, i, :, :, :]  # Extract one patch\n",
    "            patch = self.pool(torch.relu(self.conv1(patch)))  # Apply conv1 + pooling\n",
    "            patch = self.pool(torch.relu(self.conv2(patch)))  # Apply conv2 + pooling\n",
    "            patch = patch.view(patch.size(0), -1)  # Flatten the patch\n",
    "            patch_features.append(patch)\n",
    "        \n",
    "        # Stack the patch features and aggregate across patches (e.g., average)\n",
    "        patch_features = torch.stack(patch_features, dim=1)  # Shape: (batch_size, num_patches, features)\n",
    "        \n",
    "        # Aggregate features from all patches (simple average)\n",
    "        aggregated_features = patch_features.mean(dim=1)  # Average across patches\n",
    "        \n",
    "        # Pass the aggregated features through the fully connected layers\n",
    "        x = torch.relu(self.fc1(aggregated_features))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GeneExpressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneExpressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(100, 512)  # Adjust input size for gene expression data\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def prepare_data(data_dict, target_dict, validation_size=0.2, test_size=0.2):\n",
    "    # Initialize the LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Sort the patient IDs to ensure alignment between features and labels\n",
    "    patient_ids = sorted(data_dict.keys())\n",
    "    \n",
    "    # Convert dictionaries to lists of features and target labels\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    # Get all target labels (using the sorted patient IDs)\n",
    "    all_targets = [target_dict[patient_id] for patient_id in patient_ids]\n",
    "    \n",
    "    # Fit the LabelEncoder on the target labels\n",
    "    label_encoder.fit(all_targets)\n",
    "    \n",
    "    # Encode targets into integers\n",
    "    encoded_targets = label_encoder.transform(all_targets)\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        # Ensure the features are converted to tensor\n",
    "        feature_tensor = torch.tensor(data_dict[patient_id], dtype=torch.float32)\n",
    "        features.append(feature_tensor)\n",
    "        \n",
    "        # Append the corresponding encoded target label\n",
    "        targets.append(encoded_targets[patient_ids.index(patient_id)])\n",
    "\n",
    "    # Stack the features into a single tensor\n",
    "    features = torch.stack(features)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)  # Ensure target is a tensor with correct dtype\n",
    "\n",
    "    # Train-test-validation split\n",
    "    # First, split into train+val and test\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(features, targets, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Then, split train+val into train and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=validation_size / (1 - test_size), random_state=42)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 2. **Build Model for Image Modality (Fully Connected Example)**\n",
    "def train_model(model, train_data, train_labels, validation_data=None, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch:\",epoch)\n",
    "        model.train()  # Ensure the model is in training mode\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass on the training data\n",
    "        outputs = model(train_data)\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        \n",
    "        # Backward pass and optimization step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # If validation data is provided, evaluate on the validation set\n",
    "        if validation_data:\n",
    "            model.eval()  # Set the model to evaluation mode for validation\n",
    "            val_data, val_labels = validation_data\n",
    "            with torch.no_grad():  # No need to compute gradients for validation\n",
    "                val_outputs = model(val_data)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                \n",
    "                # Calculate accuracy or F1 score for validation\n",
    "                _, predicted = torch.max(val_outputs, 1)  # Get the predicted class labels\n",
    "                correct = (predicted == val_labels).sum().item()\n",
    "                val_accuracy = correct / len(val_labels)\n",
    "                \n",
    "                # Optionally, you could calculate other metrics like F1 score:\n",
    "                val_f1 = f1_score(val_labels.cpu(), predicted.cpu(), average='weighted')\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {loss.item():.4f}, \"\n",
    "                      f\"Validation Loss: {val_loss.item():.4f}, \"\n",
    "                      f\"Validation Accuracy: {val_accuracy:.4f}, \"\n",
    "                      f\"Validation F1: {val_f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {loss.item():.4f}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_data, test_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "        f1 = f1_score(test_labels.numpy(), predicted.numpy(), average='weighted')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9220b-fb2a-43f4-bba5-85927d55e4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12770/1822523262.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feature_tensor = torch.tensor(data_dict[patient_id], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input data before model: torch.Size([55, 1, 3, 256, 256])\n",
      "Shape of a single image: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch import nn, optim\n",
    "\n",
    "X_train_img, X_val_img, X_test_img, y_train_img, y_val_img, y_test_img = prepare_data(all_patient_data_image, all_patient_data)\n",
    "print(f\"Shape of input data before model: {X_train_img.shape}\")\n",
    "print(f\"Shape of a single image: {X_train_img[0].shape}\")\n",
    "# Initialize, train, and evaluate the Image Model\n",
    "image_model = ImageModel()\n",
    "trained_image_model = train_model(image_model, X_train_img, y_train_img, validation_data=(X_val_img, y_val_img))\n",
    "img_acc, img_f1 = evaluate_model(trained_image_model, X_test_img, y_test_img)\n",
    "\n",
    "print(f\"Image Modality - Accuracy: {img_acc:.4f}, F1 Score: {img_f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcbeae-fe1b-4690-b462-d53bb4bde6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. **Build Model for Gene Expression Modality (Fully Connected Example)**\n",
    "\n",
    "\n",
    "# 6. **Train Gene Expression Model**\n",
    "\n",
    "def train_gene_model(model, train_data, train_labels, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_data)\n",
    "        loss = criterion(outputs, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "\n",
    "# 7. **Evaluate Gene Expression Model**\n",
    "\n",
    "def evaluate_gene_expression_model(model, test_data, test_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(test_labels.numpy(), predicted.numpy())\n",
    "        f1 = f1_score(test_labels.numpy(), predicted.numpy(), average='weighted')\n",
    "    return accuracy, f1\n",
    "\n",
    "# Prepare Gene Expression Data\n",
    "X_train_gene, X_test_gene, y_train_gene, y_test_gene = prepare_data(all_patient_data_geneexpr, all_patient_data_class)\n",
    "\n",
    "# Initialize, train, and evaluate the Gene Expression Model\n",
    "gene_model = GeneExpressionModel()\n",
    "trained_gene_model = train_gene_model(gene_model, X_train_gene, y_train_gene)\n",
    "gene_acc, gene_f1 = evaluate_gene_expression_model(trained_gene_model, X_test_gene, y_test_gene)\n",
    "\n",
    "print(f\"Gene Expression Modality - Accuracy: {gene_acc:.4f}, F1 Score: {gene_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
